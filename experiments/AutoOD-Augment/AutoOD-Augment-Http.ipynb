{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import logging\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "def run_lof(X, y, k=60):\n",
    "    clf = LocalOutlierFactor(n_neighbors=k)\n",
    "    clf.fit(X)\n",
    "    lof_scores = -clf.negative_outlier_factor_\n",
    "    return lof_scores\n",
    "\n",
    "def get_predictions(scores, num_outliers = 400, method_name = 'LOF'):\n",
    "    threshold = np.sort(scores)[::-1][num_outliers]\n",
    "    # threshold, max_f1 = get_best_f1_score(y, lof_scores)\n",
    "    predictions = np.array(scores > threshold)\n",
    "    predictions = np.array([int(i) for i in predictions])\n",
    "#     print('F1 for {} : {}'.format(method_name, metrics.f1_score(y, predictions)))\n",
    "    return predictions, scores, metrics.f1_score(y, predictions)\n",
    "\n",
    "def get_precision_recall(scores, num_outliers = 400, method_name = 'LOF'):\n",
    "    threshold = np.sort(scores)[::-1][num_outliers]\n",
    "    # threshold, max_f1 = get_best_f1_score(y, lof_scores)\n",
    "    predictions = np.array(scores > threshold)\n",
    "    predictions = np.array([int(i) for i in predictions])\n",
    "#     print('F1 for {} : {}'.format(method_name, metrics.f1_score(y, predictions)))\n",
    "    return predictions, scores, metrics.f1_score(y, predictions), metrics.precision_score(y, predictions), metrics.recall_score(y, predictions)\n",
    "\n",
    "\n",
    "def get_best_F1(scores):\n",
    "    best_f1 = 0\n",
    "    for i in range(np.shape(scores)[0]):\n",
    "        threshold = np.sort(scores)[::-1][i]\n",
    "        predictions = np.array(scores > threshold)\n",
    "        predictions = np.array([int(i) for i in predictions])\n",
    "        cur_f1 = metrics.f1_score(y, predictions)\n",
    "        best_f1 = max(cur_f1, best_f1)\n",
    "    return best_f1\n",
    "\n",
    "def run_knn(X, y, k=60):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    knn_dists = neigh.kneighbors(X)[0][:,-1]\n",
    "    return knn_dists\n",
    "\n",
    "def run_isolation_forest(X, y, max_features = 1.0):\n",
    "    # training the model\n",
    "    clf = IsolationForest(random_state=42,max_features=max_features)\n",
    "    clf.fit(X)\n",
    "    # predictions\n",
    "    sklearn_score_anomalies = clf.decision_function(X)\n",
    "    if_scores = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
    "    return if_scores\n",
    "\n",
    "def mahalanobis(x):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(x)\n",
    "    cov = np.cov(x.T)\n",
    "    inv_covmat = sp.linalg.inv(cov)\n",
    "    results = []\n",
    "    x_minus_mu = np.array(x_minus_mu)\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        cur_data = x_minus_mu[i,:]\n",
    "        results.append(np.dot(np.dot(x_minus_mu[i,:], inv_covmat), x_minus_mu[i,:].T))\n",
    "    return np.array(results)\n",
    "#     left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "#     mahal = np.dot(left_term, x_minus_mu.T)\n",
    "#     print(mahal.diagonal())\n",
    "#     return mahal.diagonal()\n",
    "\n",
    "def run_mahalanobis(X, y):\n",
    "    # training the model\n",
    "    dist = mahalanobis(x=X)\n",
    "    return dist\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data, meta = arff.loadarff(f)\n",
    "    data = pd.DataFrame(data)\n",
    "    X = data.drop(columns=['id', 'outlier'])\n",
    "    # Map dataframe to encode values and put values into a numpy array\n",
    "    y = data[\"outlier\"].map(lambda x: 1 if x == b'yes' else 0).values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load http dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567498\n",
      "2211.0\n",
      "0.003896048972859816\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('http.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[1500, 2000, 2500, 3000, 3500, 4000]\n",
    "mahalanobis_N_range=[5000, 10000, 15000,20000, 25000, 30000]\n",
    "# mahalanobis_N_range=[10000, 15000, 20000, 25000, 30000, 35000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "\n",
    "# # remove duplicates\n",
    "# newdata = pd.DataFrame(np.concatenate((X,y), axis = 1)).drop_duplicates()\n",
    "# X = newdata[[0,1,2]].values\n",
    "# y = np.array([1 if i==1.0 else 0 for i in newdata[[3]].values])\n",
    "# print('Remove duplicates: ', len(y))\n",
    "\n",
    "# # normalize\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# transformer = Normalizer().fit(X) \n",
    "# X = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.6\n",
      "0.8\n",
      "0.9\n",
      "0.7\n",
      "Average F1s = 0.24814736698332843, Average Precision = 0.15155237556939605, Average recall = 0.9337403889642696\n",
      "Max F1s = 0.607405352933019, Precision at best F1 = 0.438, recall at best F1 = 0.9905020352781547\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "temp_if_results = dict()\n",
    "unique_if_features = list(set(if_range)) \n",
    "best_f1 = 0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "for k in unique_if_features:\n",
    "    print(k)\n",
    "    if_scores = run_isolation_forest(X, y, max_features=k)\n",
    "    temp_if_results[k] = if_scores\n",
    "for i in range(len(if_range)):\n",
    "    if_predictions, if_scores,f1, precision, recall = get_precision_recall(temp_if_results[if_range[i]], num_outliers=if_N_range[i], method_name='IF')\n",
    "    f1s.append(f1)\n",
    "    if f1>best_f1:\n",
    "        best_f1 = f1\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "print(f\"Average F1s = {sum(f1s)/len(f1s)}, Average Precision = {sum(precisions)/len(precisions)}, Average recall = {sum(recalls)/len(recalls)}\")\n",
    "print(f\"Max F1s = {max(f1s)}, Precision at best F1 = {best_precision}, recall at best F1 = {best_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "70\n",
      "40\n",
      "10\n",
      "80\n",
      "50\n",
      "20\n",
      "90\n",
      "60\n",
      "30\n",
      "100\n",
      "70\n",
      "40\n",
      "10\n",
      "80\n",
      "50\n",
      "20\n",
      "90\n",
      "60\n",
      "30\n",
      "0.5\n",
      "0.6\n",
      "0.8\n",
      "0.9\n",
      "0.7\n",
      "Best Method = Mahala, Best F1 = 0.6112883095271114\n",
      "Best F1 = 0.6112883095271114, best_precision = 0.4408, best_recall=0.9968340117593849\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_scores = []\n",
    "f1s = []\n",
    "\n",
    "method_to_bestf1 = {}\n",
    "best_f1 = 0\n",
    "best_precision=0\n",
    "best_recall=0\n",
    "\n",
    "temp_lof_results = dict()\n",
    "unique_lof_ks = list(set(lof_krange)) \n",
    "\n",
    "best_lof_f1 = 0\n",
    "best_lof_precision = 0\n",
    "best_lof_recall = 0\n",
    "for k in unique_lof_ks:\n",
    "    print(k)\n",
    "    lof_scores = run_lof(X, y, k=k)\n",
    "    temp_lof_results[k] = lof_scores\n",
    "for i in range(len(lof_krange)):\n",
    "    lof_predictions, lof_scores, f1, precision,recall = get_precision_recall(temp_lof_results[lof_krange[i]], num_outliers=N_range[i], method_name='LOF')\n",
    "    all_results.append(lof_predictions)\n",
    "    all_scores.append(lof_scores)\n",
    "    f1s.append(f1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "\n",
    "best_lof_f1 = 0\n",
    "for i in np.sort(unique_lof_ks):\n",
    "    temp_f1 = max(np.array(f1s[0:60])[np.where(np.array(lof_krange) == i)[0]])\n",
    "    best_lof_f1 = max(best_lof_f1, temp_f1)\n",
    "\n",
    "method_to_bestf1[\"LOF\"] = best_lof_f1\n",
    "\n",
    "temp_knn_results = dict()\n",
    "unique_knn_ks = list(set(knn_krange)) \n",
    "for k in unique_knn_ks:\n",
    "    print(k)\n",
    "    knn_scores = run_knn(X, y, k=k)\n",
    "    temp_knn_results[k] = knn_scores\n",
    "for i in range(len(knn_krange)):\n",
    "    knn_predictions, knn_scores,f1,precision,recall = get_precision_recall(temp_knn_results[knn_krange[i]], num_outliers=N_range[i], method_name='KNN')\n",
    "    all_results.append(knn_predictions)\n",
    "    all_scores.append(knn_scores)\n",
    "    f1s.append(f1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "best_knn_f1 = 0\n",
    "for i in np.sort(unique_knn_ks):\n",
    "    temp_f1 = max(np.array(f1s[60:120])[np.where(np.array(knn_krange) == i)[0]])\n",
    "    best_knn_f1 = max(best_knn_f1, temp_f1)\n",
    "method_to_bestf1[\"KNN\"] = best_knn_f1\n",
    "\n",
    "temp_if_results = dict()\n",
    "unique_if_features = list(set(if_range)) \n",
    "for k in unique_if_features:\n",
    "    print(k)\n",
    "    if_scores = run_isolation_forest(X, y, max_features=k)\n",
    "    temp_if_results[k] = if_scores\n",
    "for i in range(len(if_range)):\n",
    "    if_predictions, if_scores,f1,precision,recall = get_precision_recall(temp_if_results[if_range[i]], num_outliers=if_N_range[i], method_name='IF')\n",
    "    all_results.append(if_predictions)\n",
    "    all_scores.append(if_scores)\n",
    "    f1s.append(f1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "best_if_f1 = 0\n",
    "for i in np.sort(unique_if_features):\n",
    "    temp_f1 = max(np.array(f1s[120:150])[np.where(np.array(if_range) == i)[0]])\n",
    "    best_if_f1 = max(best_if_f1, temp_f1)\n",
    "method_to_bestf1[\"IF\"] = best_if_f1\n",
    "   \n",
    "mahalanobis_scores = run_mahalanobis(X, y)\n",
    "best_mahala_f1 = 0\n",
    "for i in range(len(mahalanobis_N_range)):\n",
    "    mahalanobis_predictions,mahalanobis_scores,f1,precision,recall = get_precision_recall(mahalanobis_scores, num_outliers=mahalanobis_N_range[i], method_name='mahala')\n",
    "    all_results.append(mahalanobis_predictions)\n",
    "    all_scores.append(mahalanobis_scores)\n",
    "    best_mahala_f1 = max(best_mahala_f1, f1)\n",
    "    f1s.append(f1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "method_to_bestf1[\"Mahala\"] = best_mahala_f1\n",
    "best_method = \"\"\n",
    "best_f1 =0\n",
    "for method, f1 in method_to_bestf1.items():\n",
    "    if f1 > best_f1:\n",
    "        best_method = method\n",
    "        best_f1 = f1\n",
    "\n",
    "print(f\"Best Method = {best_method}, Best F1 = {best_f1}\")\n",
    "L = np.stack(all_results).T\n",
    "scores = np.stack(all_scores).T\n",
    "print(f\"Best F1 = {best_f1}, best_precision = {best_precision}, best_recall={best_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567498, 156)\n",
      "(567498, 156)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(L))\n",
    "print(np.shape(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for MV: 0.040134967146155213\n"
     ]
    }
   ],
   "source": [
    "mid = np.shape(L)[1]/2\n",
    "predictions = np.full((len(y)), 0)\n",
    "predictions[np.sum(L, axis = 1) > mid] = 1\n",
    "print('F1 for MV:', metrics.f1_score(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_prev = L\n",
    "scores_prev = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = L_prev\n",
    "scores = scores_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567498, 156)\n"
     ]
    }
   ],
   "source": [
    "# print(max(f1s)) \n",
    "print(np.shape(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result_list = []\n",
    "classifier_result_list = []\n",
    "prediction_list = []\n",
    "cur_f1_scores = []\n",
    "prediction_high_conf_outliers = np.array([])\n",
    "prediction_high_conf_inliers = np.array([])\n",
    "prediction_classifier_disagree = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_range = np.array([[0, 60], [60, 120], [120, 150], [150, 156]])\n",
    "coef_index_range = np.array([[0, 10], [10, 20], [20, 25], [25, 26]])\n",
    "coef_remain_index = range(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 120, 121, 122, 123, 124, 150]\n",
      "(567498, 26)\n"
     ]
    }
   ],
   "source": [
    "scores_for_training_indexes = []\n",
    "for i in range(len(index_range)):\n",
    "    start=index_range[i][0]\n",
    "    temp_range = coef_index_range[i][1]-coef_index_range[i][0]\n",
    "    scores_for_training_indexes  = scores_for_training_indexes + list(range(start, start+temp_range))\n",
    "print(scores_for_training_indexes) \n",
    "scores_for_training = scores[:, np.array(scores_for_training_indexes)]\n",
    "print(np.shape(scores_for_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative train LR and classifier(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0, L shape = (567498, 156)\n",
      "All agree, Number of outliers = 7\n",
      "All agree, Number of inliers = 405653\n",
      "num of inliers = 405653\n",
      "num of outliers = 7\n",
      "num of outliers = 5600\n",
      "Training data shape:  (411253, 26)\n",
      "Training data F-1 0.030942334739803096\n",
      "F-1 score from LR: 0.0009445606403962393\n",
      "(411253, 3)\n",
      "(411253,)\n",
      "F-1 score from SVM: 0.21111429389859637\n",
      "length of prediction_high_conf_outliers: 170\n",
      "length of prediction high conf inliers:  3894\n",
      "0\n",
      "[ 1  2  3  4  7  8  9 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[[ 0  7]\n",
      " [ 7 14]\n",
      " [14 19]\n",
      " [19 19]]\n",
      "[[  0  42]\n",
      " [ 42  84]\n",
      " [ 84 114]\n",
      " [114 114]]\n",
      "##################################################################\n",
      "Iteration = 1, L shape = (567498, 114)\n",
      "All agree, Number of outliers = 7\n",
      "All agree, Number of inliers = 445311\n",
      "num of inliers = 2680\n",
      "num of outliers = 170\n",
      "num of outliers = 170\n",
      "Training data shape:  (2850, 19)\n",
      "Training data F-1 0.7453874538745386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0009791406439451132\n",
      "(2850, 3)\n",
      "(2850,)\n",
      "F-1 score from SVM: 0.3232928790758883\n",
      "length of prediction_high_conf_outliers: 171\n",
      "length of prediction high conf inliers:  54162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[2 3 4 9]\n",
      "[[0 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]]\n",
      "[[ 0 24]\n",
      " [24 24]\n",
      " [24 24]\n",
      " [24 24]]\n",
      "##################################################################\n",
      "Iteration = 2, L shape = (567498, 24)\n",
      "All agree, Number of outliers = 162\n",
      "All agree, Number of inliers = 496982\n",
      "num of inliers = 43157\n",
      "num of outliers = 291\n",
      "num of outliers = 174\n",
      "Training data shape:  (43331, 4)\n",
      "Training data F-1 0.7615658362989323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0012045022065670676\n",
      "(43331, 3)\n",
      "(43331,)\n",
      "F-1 score from SVM: 0.36142214957090313\n",
      "length of prediction_high_conf_outliers: 172\n",
      "length of prediction high conf inliers:  0\n",
      "Coef from normal training:  [7.04526931e-09 3.04769167e-07 3.29320955e-07 2.88137157e-07]\n",
      "Combined Coef:  [7.04526931e-09 3.04769167e-07 3.29320955e-07 2.88137157e-07]\n",
      "1.0143428942784396e-07\n",
      "[3 4 9]\n",
      "[[0 3]\n",
      " [3 3]\n",
      " [3 3]\n",
      " [3 3]]\n",
      "[[ 0 18]\n",
      " [18 18]\n",
      " [18 18]\n",
      " [18 18]]\n",
      "##################################################################\n",
      "Iteration = 3, L shape = (567498, 18)\n",
      "All agree, Number of outliers = 360\n",
      "All agree, Number of inliers = 506924\n",
      "num of inliers = 506924\n",
      "num of outliers = 455\n",
      "num of outliers = 175\n",
      "Training data shape:  (368525, 3)\n",
      "Training data F-1 0.7588652482269503\n",
      "F-1 score from LR: 0.007412795732446569\n",
      "(368525, 3)\n",
      "(368525,)\n",
      "F-1 score from SVM: 0.9142030183998345\n",
      "length of prediction_high_conf_outliers: 123\n",
      "length of prediction high conf inliers:  538312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[4 9]\n",
      "[[0 2]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 2]]\n",
      "[[ 0 12]\n",
      " [12 12]\n",
      " [12 12]\n",
      " [12 12]]\n",
      "##################################################################\n",
      "Iteration = 4, L shape = (567498, 12)\n",
      "All agree, Number of outliers = 600\n",
      "All agree, Number of inliers = 516000\n",
      "num of inliers = 508145\n",
      "num of outliers = 635\n",
      "num of outliers = 123\n",
      "Training data shape:  (508268, 2)\n",
      "Training data F-1 0.9304347826086956\n",
      "F-1 score from LR: 0.01048300186146762\n",
      "(508268, 3)\n",
      "(508268,)\n",
      "F-1 score from SVM: 0.993676603432701\n",
      "length of prediction_high_conf_outliers: 123\n",
      "length of prediction high conf inliers:  547181\n",
      "0.008734516684065937\n",
      "[4]\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[[0 6]\n",
      " [6 6]\n",
      " [6 6]\n",
      " [6 6]]\n",
      "##################################################################\n",
      "Iteration = 5, L shape = (567498, 6)\n",
      "All agree, Number of outliers = 4998\n",
      "All agree, Number of inliers = 537498\n",
      "num of inliers = 533969\n",
      "num of outliers = 5033\n",
      "num of outliers = 123\n",
      "Training data shape:  (534092, 1)\n",
      "Training data F-1 0.9264069264069263\n",
      "F-1 score from LR: 0.009934750578825511\n",
      "(534092, 3)\n",
      "(534092,)\n",
      "F-1 score from SVM: 0.993676603432701\n",
      "length of prediction_high_conf_outliers: 122\n",
      "length of prediction high conf inliers:  541169\n",
      "##################################################################\n",
      "Iteration = 6, L shape = (567498, 6)\n",
      "All agree, Number of outliers = 4998\n",
      "All agree, Number of inliers = 537498\n",
      "num of inliers = 535406\n",
      "num of outliers = 5032\n",
      "num of outliers = 122\n",
      "Training data shape:  (535528, 1)\n",
      "Training data F-1 0.9257641921397379\n",
      "F-1 score from LR: 0.010411194421892164\n",
      "(535528, 3)\n",
      "(535528,)\n",
      "F-1 score from SVM: 0.9930038366057323\n",
      "length of prediction_high_conf_outliers: 122\n",
      "length of prediction high conf inliers:  545602\n",
      "##################################################################\n",
      "Iteration = 7, L shape = (567498, 6)\n",
      "All agree, Number of outliers = 4998\n",
      "All agree, Number of inliers = 537498\n",
      "num of inliers = 535406\n",
      "num of outliers = 5032\n",
      "num of outliers = 122\n",
      "Training data shape:  (535528, 1)\n",
      "Training data F-1 0.9257641921397379\n",
      "F-1 score from LR: 0.010411194421892164\n",
      "(535528, 3)\n",
      "(535528,)\n",
      "F-1 score from SVM: 0.9930038366057323\n",
      "length of prediction_high_conf_outliers: 122\n",
      "length of prediction high conf inliers:  545602\n",
      "##################################################################\n",
      "Iteration = 8, L shape = (567498, 6)\n",
      "All agree, Number of outliers = 4998\n",
      "All agree, Number of inliers = 537498\n",
      "num of inliers = 535406\n",
      "num of outliers = 5032\n",
      "num of outliers = 122\n",
      "Training data shape:  (535528, 1)\n",
      "Training data F-1 0.9257641921397379\n",
      "F-1 score from LR: 0.010411194421892164\n",
      "(535528, 3)\n",
      "(535528,)\n",
      "F-1 score from SVM: 0.9930038366057323\n",
      "length of prediction_high_conf_outliers: 122\n",
      "length of prediction high conf inliers:  545602\n",
      "##################################################################\n",
      "Iteration = 9, L shape = (567498, 6)\n",
      "All agree, Number of outliers = 4998\n",
      "All agree, Number of inliers = 537498\n",
      "num of inliers = 535406\n",
      "num of outliers = 5032\n",
      "num of outliers = 122\n",
      "Training data shape:  (535528, 1)\n",
      "Training data F-1 0.9257641921397379\n",
      "F-1 score from LR: 0.010411194421892164\n",
      "(535528, 3)\n",
      "(535528,)\n",
      "F-1 score from SVM: 0.9930038366057323\n",
      "length of prediction_high_conf_outliers: 122\n",
      "length of prediction high conf inliers:  545602\n",
      "##################################################################\n",
      "Iteration = 10, L shape = (567498, 6)\n",
      "All agree, Number of outliers = 4998\n",
      "All agree, Number of inliers = 537498\n",
      "num of inliers = 535406\n",
      "num of outliers = 5032\n",
      "num of outliers = 122\n",
      "Training data shape:  (535528, 1)\n",
      "Training data F-1 0.9257641921397379\n",
      "F-1 score from LR: 0.010411194421892164\n",
      "(535528, 3)\n",
      "(535528,)\n",
      "F-1 score from SVM: 0.9930038366057323\n",
      "length of prediction_high_conf_outliers: 122\n",
      "length of prediction high conf inliers:  545602\n"
     ]
    }
   ],
   "source": [
    "# stable version\n",
    "high_confidence_threshold = 0.99\n",
    "low_confidence_threshold = 0.01\n",
    "max_iter = 500\n",
    "remain_params_tracking = np.array(range(0,np.max(coef_index_range)))\n",
    "training_data_F1 = []\n",
    "two_prediction_corr = []\n",
    "\n",
    "min_max_diff = []\n",
    "N_size = 6\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "for i_range in range(0, 50):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}, L shape = {}'.format(i_range, np.shape(L)))\n",
    "    num_methods = np.shape(L)[1]\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "    print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "    print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "\n",
    "    all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    if len(prediction_high_conf_inliers) > 0:\n",
    "        all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "    \n",
    "    self_agree_index_list = []\n",
    "    if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "        for i in range(0, len(index_range)):\n",
    "            if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "                continue\n",
    "            temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "            self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "        self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "    all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "    all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training)\n",
    "    scores_transformed = transformer.transform(scores_for_training)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "    print('Training data shape: ', np.shape(training_data))\n",
    "    training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "    print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "#     clf = SVC(gamma='auto', probability=True, random_state=0)\n",
    "#     clf.fit(training_data, labels)\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "    print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > 0.5])))\n",
    "    \n",
    "    \n",
    "    transformer = RobustScaler().fit(X)\n",
    "    X_transformed = transformer.transform(X)\n",
    "    X_training_data = X_transformed[data_indexes]\n",
    "    print(np.shape(X_training_data))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_training_data, labels)\n",
    "    clf_predictions_X = clf_X.predict(X_transformed)\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "        \n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    classifier_result_list.append(clf_predict_proba_X)\n",
    "    \n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "    \n",
    "    prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "    print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "    print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "    \n",
    "    temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > 0.5])\n",
    "    temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "    prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "#     print('length of prediction-classifier disagree: {}'.format(len(prediction_classifier_disagree)))\n",
    "#     print('length of prediction-classifier disagree in training: {}'.format(len(np.where(temp_prediction[data_indexes] != temp_classifier[data_indexes])[0])))\n",
    "#     print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))\n",
    "    two_prediction_corr.append(np.corrcoef(clf_predict_proba,clf_predict_proba_X)[0,1])\n",
    "\n",
    "    if np.max(coef_index_range) >= 2:\n",
    "        if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "            new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "            new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "            new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "            clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "            combined_coef = clf_prune_2.coef_[0]  \n",
    "        else:\n",
    "            print('Coef from normal training: ', clf.coef_[0])\n",
    "            combined_coef = clf.coef_[0]\n",
    "            print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "        if(np.max(coef_index_range) >= 2):\n",
    "            if(len(set(combined_coef)) > 1):\n",
    "                cur_clf_coef = combined_coef \n",
    "                cutoff = max(max(0, np.mean(combined_coef)-np.std(combined_coef)),min(combined_coef))\n",
    "                print(cutoff)\n",
    "\n",
    "                remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "                remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "                print(remain_params_tracking)\n",
    "                remain_indexes_after_cond_expanded = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                    s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                    saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "                    for j in range(N_size):\n",
    "                        remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "                new_coef_index_range_seq = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "                coef_index_range = []\n",
    "                index_range = []\n",
    "                cur_sum = 0\n",
    "                for i in range(0, len(new_coef_index_range_seq)):\n",
    "                    coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                    index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                    cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "                coef_index_range = np.array(coef_index_range)\n",
    "                index_range = np.array(index_range)\n",
    "                print(coef_index_range)\n",
    "                print(index_range)\n",
    "\n",
    "                L=L[:,remain_indexes_after_cond_expanded]\n",
    "                scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "    if((len(last_training_data_indexes) == len(data_indexes)) and \n",
    "       (sum(last_training_data_indexes == data_indexes) == len(data_indexes)) and \n",
    "       (np.max(coef_index_range) < 2)):\n",
    "        counter =  counter + 1\n",
    "    else:\n",
    "        counter = 0\n",
    "    if(counter > 3):\n",
    "        break\n",
    "    last_training_data_indexes = data_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data outlier F-1', metrics.f1_score(y[all_outlier_indexes], labels[labels==1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
